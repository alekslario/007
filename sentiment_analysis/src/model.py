# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iyFTu0kXGAC_XJzU1dqcl9qpdeqyyU8B
"""

import numpy as np

import tensorflow as tf
from keras import models
from keras import layers
from keras.preprocessing.text import Tokenizer
from keras.utils import to_categorical
import json

from google.colab import files
uploaded = files.upload()

def vectorize(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1
    return results

with open('output.json', 'r', encoding='utf-8') as file:
    app_data = json.load(file)

# Initialize lists to store all reviews and labels
texts = []
labels = []

# Loop through each app
for app in app_data:
    app_reviews = app["reviews"]

    # Filter reviews with a rating of 3
    filtered_reviews = [review for review in app_reviews if review.get('rating') != 3 and 'review' in review]

    # Extract review text and labels from filtered reviews
    texts_partial = [review['review'] for review in filtered_reviews]
    labels_partial = [1 if review['rating'] > 3 else 0 for review in filtered_reviews]

    # Append to the lists
    texts.extend(texts_partial)
    labels.extend(labels_partial)

# Tokenize the text data
max_words = 10000
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# Vectorize the text data
data = vectorize(sequences)

# Convert labels to numpy array
labels = np.asarray(labels).astype("float32")

# Split the data into training and testing sets
test_size = 200
test_data = data[:test_size]
test_labels = labels[:test_size]
train_data = data[test_size:]
train_labels = labels[test_size:]

# Define the model
model = models.Sequential()
model.add(layers.Dense(50, activation="relu", input_shape=(max_words,)))
model.add(layers.Dropout(0.3))
model.add(layers.Dense(50, activation="relu"))
model.add(layers.Dropout(0.2))
model.add(layers.Dense(50, activation="relu"))
model.add(layers.Dense(1, activation="sigmoid"))
model.summary()

# Compile the model
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

# Train the model
results = model.fit(
    train_data, train_labels,
    epochs=2,
    batch_size=32,
    validation_data=(test_data, test_labels)
)

import matplotlib.pyplot as plt
# Plot training and validation loss
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(results.history['loss'], label='Training Loss')
plt.plot(results.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(results.history['accuracy'], label='Training Accuracy')
plt.plot(results.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

weights = model.layers[0].get_weights()[0]

# Flatten the weights array
flattened_weights = weights.flatten()

# Get the indices of the top 30 most positive and most negative words
top_30_positive_indices = np.argsort(flattened_weights)[-30:][::-1]
top_30_negative_indices = np.argsort(flattened_weights)[:30]

# Ensure the indices are within the expected range and get corresponding words
vocab_size = min(max_words, len(reverse_word_index))
top_30_positive_words = [reverse_word_index.get(index % vocab_size, "Not found in vocabulary") for index in top_30_positive_indices]
top_30_negative_words = [reverse_word_index.get(index % vocab_size, "Not found in vocabulary") for index in top_30_negative_indices]

# Print the top 30 most positive and most negative words
print("Top 30 most positive words:")
for i, word in enumerate(top_30_positive_words, 1):
    print(f"{i}. {word}")

print("\nTop 30 most negative words:")
for i, word in enumerate(top_30_negative_words, 1):
    print(f"{i}. {word}")